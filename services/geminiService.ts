import { GoogleGenAI, SchemaType, Type } from "@google/genai";
import { Novel, Chapter, KnowledgeEntry } from '../types';
import { incrementUsageStats } from './storageService';

// Initialize the client (Global instance for standard calls)
const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });

const MODEL_NAME = 'gemini-3-pro-preview';
const IMAGE_MODEL_NAME = 'gemini-2.5-flash-image'; 

const SYSTEM_INSTRUCTION = `
ä½ æ˜¯ä¸€ä½ä¸“å®¶çº§çš„å°è¯´å®¶å’Œåˆ›æ„å†™ä½œåŠ©æ‰‹ã€‚
è¯·ç”¨é«˜è´¨é‡ã€å¼•äººå…¥èƒœä¸”æå†™ç»†è…»çš„ç®€ä½“ä¸­æ–‡è¿›è¡Œå†™ä½œã€‚
ä½ çš„ç›®æ ‡æ˜¯å¸®åŠ©ç”¨æˆ·æ’°å†™å°è¯´ï¼Œç”Ÿæˆç« èŠ‚ã€å¤§çº²æˆ–ç»­å†™åœºæ™¯ã€‚
ä¿æŒè¯­æ°”ã€è§’è‰²å£°éŸ³å’Œæƒ…èŠ‚èµ°å‘çš„ä¸€è‡´æ€§ã€‚
`;

/**
 * Generates a continuation or a new chapter based on context.
 */
export const generateStorySegment = async (
  novel: Novel,
  currentChapterIndex: number,
  prompt: string,
  referenceContent: string[], // Selected knowledge base content
  targetWordCount: number | undefined, // New: Target word count
  onStream: (text: string) => void
): Promise<string> => {
  
  // Build context from previous chapters (summary or raw text if short)
  const previousChapters = novel.chapters.slice(Math.max(0, currentChapterIndex - 2), currentChapterIndex);
  
  let contextString = `å°è¯´æ ‡é¢˜: ${novel.title}\nç±»å‹: ${novel.genre}\nç®€ä»‹/æ¢—æ¦‚: ${novel.description}\n\n`;
  
  let knowledgeConstraint = "";

  if (referenceContent.length > 0) {
      contextString += `ã€ğŸ“š æ ¸å¿ƒè®¾å®šä¸çŸ¥è¯†åº“ (å¿…é¡»éµå¾ª)ã€‘:\n${referenceContent.join('\n\n')}\n\n`;
      
      knowledgeConstraint = `
      ã€âš ï¸ çŸ¥è¯†åº“ä½¿ç”¨çº¦æŸ (é‡è¦)ã€‘:
      1. **ä¸¥æ ¼ä¸€è‡´æ€§**: ä½ ç”Ÿæˆçš„æ¯ä¸€ä¸ªå­—éƒ½å¿…é¡»ä¸¥æ ¼éµå®ˆä¸Šè¿°ã€æ ¸å¿ƒè®¾å®šä¸çŸ¥è¯†åº“ã€‘ä¸­çš„äº‹å®ã€‚
      2. **ä¸¥ç¦å†²çª**: ç»å¯¹ä¸è¦åˆ›é€ ä¸ä¸Šè¿°è®¾å®šç›¸çŸ›ç›¾çš„æƒ…èŠ‚æˆ–æè¿°ã€‚
      3. **æ·±åº¦æ•´åˆ**: è¯·ç§¯æåˆ©ç”¨ä¸Šè¿°èµ„æ–™ä¸­çš„ç»†èŠ‚æ¥å¢å¼ºæ–‡ç« çš„è¿è´¯æ€§å’Œæ²‰æµ¸æ„Ÿã€‚
      `;
  }

  if (previousChapters.length > 0) {
    contextString += `å‰æƒ…å›é¡¾:\n`;
    previousChapters.forEach((chap, idx) => {
       contextString += `ç¬¬ ${idx + 1} ç« : ${chap.summary || chap.content.substring(0, 500) + '...'}\n`;
    });
  }

  let lengthInstruction = "";
  if (targetWordCount && targetWordCount > 0) {
      lengthInstruction = `\n\nã€å­—æ•°è¦æ±‚ã€‘ï¼šè¯·ç”Ÿæˆå¤§çº¦ ${targetWordCount} ä¸ªä¸­æ–‡å­—ç¬¦çš„å†…å®¹ã€‚å°½é‡è´´è¿‘è¿™ä¸ªå­—æ•°ï¼Œä¸è¦è¿‡çŸ­æˆ–è¿‡é•¿ã€‚`;
  }

  const finalPrompt = `
${contextString}

${knowledgeConstraint}

å½“å‰ä»»åŠ¡:
${prompt}
${lengthInstruction}

è¯·ç”¨ä¸­æ–‡æ’°å†™ã€‚
`;

  try {
    const responseStream = await ai.models.generateContentStream({
      model: MODEL_NAME,
      contents: [{ role: 'user', parts: [{ text: finalPrompt }] }],
      config: {
        systemInstruction: SYSTEM_INSTRUCTION,
        temperature: 0.8,
      },
    });

    let fullText = '';
    for await (const chunk of responseStream) {
      const text = chunk.text;
      if (text) {
        fullText += text;
        onStream(text);
      }
      // Track Usage Metadata if available (usually in last chunk)
      if (chunk.usageMetadata) {
         incrementUsageStats(chunk.usageMetadata.promptTokenCount || 0, chunk.usageMetadata.candidatesTokenCount || 0);
      }
    }
    return fullText;
  } catch (error) {
    console.error("Gemini API Error:", error);
    throw error;
  }
};

/**
 * Analyzes the user's prompt and story context to intelligently select relevant knowledge entries.
 */
export const recommendRelevantKnowledge = async (
    prompt: string,
    novelTitle: string,
    outline: string,
    knowledgeIndex: { id: string; title: string; category: string }[]
): Promise<string[]> => {
    try {
        const indexStr = knowledgeIndex.map(k => `- [${k.category}] ${k.title} (ID: ${k.id})`).join('\n');

        const analysisPrompt = `
        ä½ æ˜¯ä¸€ä¸ªâ€œä¸Šä¸‹æ–‡æ£€ç´¢åŠ©æ‰‹â€ã€‚
        å°è¯´ä¿¡æ¯ï¼šæ ‡é¢˜ï¼š${novelTitle}
        å¤§çº²/èƒŒæ™¯ï¼š${outline.substring(0, 500)}...
        ç”¨æˆ·æŒ‡ä»¤ï¼š "${prompt}"
        ã€å¯ç”¨çŸ¥è¯†åº“ç´¢å¼•ã€‘ï¼š${indexStr}
        ä»»åŠ¡ï¼šåˆ¤æ–­éœ€è¦å¼•ç”¨å“ªäº›çŸ¥è¯†åº“æ¡ç›®ï¼Ÿä»…è¿”å›IDæ•°ç»„ã€‚
        `;

        const response = await ai.models.generateContent({
            model: MODEL_NAME,
            contents: analysisPrompt,
            config: {
                responseMimeType: "application/json",
                temperature: 0.1,
            }
        });

        // Track Usage
        if (response.usageMetadata) {
            incrementUsageStats(response.usageMetadata.promptTokenCount || 0, response.usageMetadata.candidatesTokenCount || 0);
        }

        const jsonText = response.text;
        if (!jsonText) return [];
        
        return JSON.parse(jsonText) as string[];
    } catch (e) {
        console.error("Smart context selection failed", e);
        return [];
    }
};

export const generateSummary = async (chapterContent: string): Promise<string> => {
    try {
        const response = await ai.models.generateContent({
            model: MODEL_NAME,
            contents: `è¯·ç”¨3-4å¥è¯æ€»ç»“ä»¥ä¸‹ç« èŠ‚å†…å®¹ï¼ˆä½¿ç”¨ä¸­æ–‡ï¼‰ï¼Œä½œä¸ºæœªæ¥å†™ä½œçš„ä¸Šä¸‹æ–‡ï¼š\n\n${chapterContent}`,
        });
        
        if (response.usageMetadata) {
            incrementUsageStats(response.usageMetadata.promptTokenCount || 0, response.usageMetadata.candidatesTokenCount || 0);
        }
        
        return response.text || "";
    } catch (e) {
        console.error("Summary generation failed", e);
        return "";
    }
}

export const generateIdeas = async (topic: string): Promise<string> => {
     try {
        const response = await ai.models.generateContent({
            model: MODEL_NAME,
            contents: `åŸºäºä¸»é¢˜â€œ${topic}â€ï¼Œæä¾›3ä¸ªå¯Œæœ‰åˆ›æ„çš„å°è¯´æ ‡é¢˜å’Œä¸€å¥è¯çš„ç®€çŸ­é’©å­ï¼ˆHookï¼‰ã€‚è¯·ä»¥ç®€å•çš„åˆ—è¡¨å½¢å¼ç”¨ä¸­æ–‡è¿”å›ã€‚`,
        });

        if (response.usageMetadata) {
            incrementUsageStats(response.usageMetadata.promptTokenCount || 0, response.usageMetadata.candidatesTokenCount || 0);
        }

        return response.text || "";
    } catch (e) {
        return "";
    }
}

export const analyzeStoryConsistency = async (
    textToAnalyze: string,
    entries: KnowledgeEntry[],
    categoryMap: Record<string, string>
): Promise<string> => {
    try {
        const context = entries.map(e =>
            `ã€${categoryMap[e.categoryId] || 'è®¾å®š'}ã€‘ ${e.title}:\n${e.content}`
        ).join('\n\n');

        const prompt = `
        ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å°è¯´è¿è´¯æ€§ç¼–è¾‘ã€‚è¯·åˆ†æä»¥ä¸‹â€œç”Ÿæˆæ–‡æœ¬â€ï¼Œå°†å…¶ä¸â€œçŸ¥è¯†åº“è®¾å®šâ€è¿›è¡Œå¯¹æ¯”ã€‚
        æ£€æŸ¥äººç‰©OOCã€ä¸–ç•Œè§‚å†²çªã€‚
        
        çŸ¥è¯†åº“è®¾å®šï¼š${context}
        ç”Ÿæˆæ–‡æœ¬ï¼š${textToAnalyze}
        
        è¾“å‡ºç®€ç»ƒçš„åˆ†ææŠ¥å‘Šã€‚
        `;

        const response = await ai.models.generateContent({
            model: MODEL_NAME,
            contents: prompt,
            config: {
                temperature: 0.2,
            }
        });

        if (response.usageMetadata) {
            incrementUsageStats(response.usageMetadata.promptTokenCount || 0, response.usageMetadata.candidatesTokenCount || 0);
        }

        return response.text || "æ— æ³•ç”Ÿæˆåˆ†ææŠ¥å‘Šã€‚";
    } catch (e) {
        console.error("Consistency check failed", e);
        return "åˆ†æå¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥ã€‚";
    }
};

export const fixStoryConsistency = async (
    originalText: string,
    consistencyReport: string,
    entries: KnowledgeEntry[],
    categoryMap: Record<string, string>
): Promise<string> => {
    try {
        const context = entries.map(e =>
            `ã€${categoryMap[e.categoryId] || 'è®¾å®š'}ã€‘ ${e.title}:\n${e.content}`
        ).join('\n\n');

        const prompt = `
        ä½ æ˜¯ä¸€ä½èµ„æ·±çš„å°è¯´ä¿®è®¢ç¼–è¾‘ã€‚
        è¯·æ ¹æ®æä¾›çš„ã€æ£€æŸ¥æŠ¥å‘Šã€‘ï¼Œé‡å†™ã€åŸå§‹æ–‡æœ¬ã€‘ï¼Œä¿®æ­£å…¶ä¸­çš„é€»è¾‘é”™è¯¯ã€äººç‰©OOCæˆ–è®¾å®šå†²çªã€‚
        
        ã€çŸ¥è¯†åº“è®¾å®š (å‚è€ƒ)ã€‘ï¼š
        ${context}
        
        ã€æ£€æŸ¥æŠ¥å‘Š (éœ€ä¿®æ­£çš„é—®é¢˜)ã€‘ï¼š
        ${consistencyReport}
        
        ã€åŸå§‹æ–‡æœ¬ã€‘ï¼š
        ${originalText}
        
        è¦æ±‚ï¼š
        1. ä¿®æ­£æ‰€æœ‰æŠ¥å‘Šä¸­æŒ‡å‡ºçš„é—®é¢˜ã€‚
        2. ä¿æŒåŸæœ‰çš„å™äº‹é£æ ¼å’Œæµç•…åº¦ã€‚
        3. ç›´æ¥è¾“å‡ºä¿®æ­£åçš„æ­£æ–‡ï¼Œä¸éœ€è¦é¢å¤–çš„è§£é‡Šã€‚
        `;

        const response = await ai.models.generateContent({
            model: MODEL_NAME,
            contents: prompt,
            config: {
                temperature: 0.7,
            }
        });

        if (response.usageMetadata) {
            incrementUsageStats(response.usageMetadata.promptTokenCount || 0, response.usageMetadata.candidatesTokenCount || 0);
        }

        return response.text || originalText;
    } catch (e) {
        console.error("Consistency fix failed", e);
        throw e;
    }
};

export interface KnowledgeUpdateSuggestion {
    name: string;
    description: string;
    type: 'NEW' | 'UPDATE';
    categoryType: 'CHARACTER' | 'WORLD' | 'ITEM' | 'OTHER';
    reason: string;
    originalId?: string; 
}

export const analyzeStoryEvolution = async (
    chapterContent: string,
    existingEntries: KnowledgeEntry[],
    categoryMap: Record<string, string>
): Promise<KnowledgeUpdateSuggestion[]> => {
    try {
        const contextString = existingEntries.map(e => 
            `ID: ${e.id} | Type: ${categoryMap[e.categoryId]} | Name: ${e.title}\nSummary: ${e.content.substring(0, 100)}...`
        ).join('\n---\n');

        const prompt = `
        ä½œä¸ºå°è¯´è®¾å®šæ•´ç†åŠ©æ‰‹ï¼Œè¯·é˜…è¯»ã€æœ€æ–°ç« èŠ‚ã€‘ï¼Œå¯¹æ¯”ã€ç°æœ‰çŸ¥è¯†åº“ã€‘ï¼Œæ•æ‰æ–°å‡ºç°çš„æˆ–å‘ç”Ÿå˜åŒ–çš„é‡è¦å…ƒç´ ï¼ˆäººç‰©ã€ä¸–ç•Œè§‚ã€ç‰©å“ï¼‰ã€‚

        ã€ç°æœ‰çŸ¥è¯†åº“æ‘˜è¦ã€‘:
        ${contextString}

        ã€æœ€æ–°ç« èŠ‚å†…å®¹ã€‘:
        ${chapterContent}

        è¯·è¿”å›ä¸€ä¸ª JSON æ•°ç»„ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
        [
          {
            "name": "æ¡ç›®åç§°",
            "description": "æ–°çš„å®Œæ•´è®¾å®šæè¿°ï¼ˆåŒ…å«æ—§ä¿¡æ¯å’Œæ–°å˜åŒ–ï¼‰",
            "type": "NEW" | "UPDATE",
            "categoryType": "CHARACTER" | "WORLD" | "ITEM" | "OTHER",
            "reason": "å˜æ›´ç†ç”±",
            "originalId": "å¦‚æœæ˜¯UPDATEï¼Œå¿…é¡»å¡«å…¥ç°æœ‰çŸ¥è¯†åº“çš„ID" 
          }
        ]
        
        å¦‚æœæ²¡æœ‰ä»»ä½•å€¼å¾—è®°å½•çš„è®¾å®šï¼Œè¿”å›ç©ºæ•°ç»„ []ã€‚
        `;

        const response = await ai.models.generateContent({
            model: MODEL_NAME,
            contents: prompt,
            config: {
                responseMimeType: "application/json",
                temperature: 0.3,
            }
        });

        if (response.usageMetadata) {
            incrementUsageStats(response.usageMetadata.promptTokenCount || 0, response.usageMetadata.candidatesTokenCount || 0);
        }

        const jsonText = response.text;
        if (!jsonText) return [];

        try {
            const result = JSON.parse(jsonText) as KnowledgeUpdateSuggestion[];
            return result;
        } catch (parseError) {
            return [];
        }

    } catch (e) {
        console.error("Story evolution analysis failed", e);
        return [];
    }
};


export const generateBookCover = async (
    title: string,
    description: string,
    style: string,
    genre: string
): Promise<string | null> => {
    
    try {
        const prompt = `
        A high quality book cover for a novel. Title: "${title}", Genre: ${genre}, Style: ${style}.
        Scene: ${description.substring(0, 300)}
        No text on image.
        `;

        // Reverted to generateContent for gemini-2.5-flash-image
        // This avoids 404 issues with Imagen 3 if not provisioned
        const response = await ai.models.generateContent({
            model: IMAGE_MODEL_NAME,
            contents: {
                parts: [{ text: prompt }]
            },
            config: {
                imageConfig: {
                    aspectRatio: "1:1"
                }
            }
        });

        // Iterate through parts to find the image
        if (response.candidates && response.candidates.length > 0) {
            for (const part of response.candidates[0].content.parts) {
                if (part.inlineData && part.inlineData.data) {
                    return part.inlineData.data;
                }
            }
        }
        
        return null;
    } catch (e) {
        console.error("Image generation failed", e);
        throw e;
    }
}

export const generatePromptTemplate = async (
    userIntent: string,
    category: string
): Promise<string> => {
    try {
        const prompt = `
        ä½œä¸ºæç¤ºè¯å·¥ç¨‹å¸ˆï¼Œè¯·ä¸ºâ€œ${category}â€åˆ†ç±»ç¼–å†™ä¸€ä¸ª Prompt æ¨¡æ¿ã€‚
        ç”¨æˆ·éœ€æ±‚ï¼šâ€œ${userIntent}â€ã€‚
        `;

        const response = await ai.models.generateContent({
            model: MODEL_NAME,
            contents: prompt,
            config: { temperature: 0.7 }
        });

        if (response.usageMetadata) {
             incrementUsageStats(response.usageMetadata.promptTokenCount || 0, response.usageMetadata.candidatesTokenCount || 0);
        }

        return response.text?.trim() || "";
    } catch (e) {
        throw e;
    }
};

export const optimizePromptTemplate = async (
    currentContent: string,
    instruction: string
): Promise<string> => {
    try {
        const prompt = `
        ä¼˜åŒ–ä»¥ä¸‹æç¤ºè¯ã€‚
        ç°æœ‰ï¼šâ€œ${currentContent}â€
        å»ºè®®ï¼šâ€œ${instruction}â€
        `;

        const response = await ai.models.generateContent({
            model: MODEL_NAME,
            contents: prompt,
            config: { temperature: 0.6 }
        });

        if (response.usageMetadata) {
             incrementUsageStats(response.usageMetadata.promptTokenCount || 0, response.usageMetadata.candidatesTokenCount || 0);
        }

        return response.text?.trim() || "";
    } catch (e) {
        throw e;
    }
}

export const expandKnowledgeEntry = async (
    currentContent: string,
    userPrompt: string
): Promise<string> => {
    try {
        const prompt = `
        ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è®¾å®šæ‰©å……åŠ©æ‰‹ã€‚
        
        ã€å½“å‰è®¾å®šæ–‡æ¡£å†…å®¹ã€‘ï¼š
        ${currentContent}
        
        ã€ç”¨æˆ·æŒ‡ä»¤ã€‘ï¼š
        ${userPrompt}
        
        è¯·æ ¹æ®å½“å‰æ–‡æ¡£å†…å®¹å’Œç”¨æˆ·æŒ‡ä»¤ï¼Œç”Ÿæˆæ–°çš„è®¾å®šå†…å®¹ã€‚
        å¦‚æœç”¨æˆ·æŒ‡ä»¤æ˜¯å…³äºæ‰©å†™ï¼Œè¯·ä¸°å¯Œç»†èŠ‚ã€‚
        å¦‚æœç”¨æˆ·æŒ‡ä»¤æ˜¯å…³äºè¡ç”Ÿï¼ˆä¾‹å¦‚åˆ›é€ ç›¸å…³äººç‰©æˆ–ç‰©å“ï¼‰ï¼Œè¯·ç”Ÿæˆæ–°çš„å®ä½“æè¿°ã€‚
        è¯·ç›´æ¥è¿”å›å†…å®¹ï¼Œæ— éœ€å¯’æš„ã€‚
        `;

        const response = await ai.models.generateContent({
            model: MODEL_NAME,
            contents: prompt,
            config: { temperature: 0.7 }
        });

         if (response.usageMetadata) {
            incrementUsageStats(response.usageMetadata.promptTokenCount || 0, response.usageMetadata.candidatesTokenCount || 0);
        }

        return response.text?.trim() || "";
    } catch (e) {
        console.error("Expand knowledge failed", e);
        throw e;
    }
};